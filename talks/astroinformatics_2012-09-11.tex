% Copyright 2012 David W. Hogg (NYU).  All rights reserved.

\documentclass[pdftex]{beamer}
\input{./hogg_presentation} % hogg standard colors
\usepackage{amssymb,amsmath,mathrsfs}

\title{Going beyond map--reduce and \\ going beyond maximum-likelihood}
\author[David W. Hogg (NYU)]{David W. Hogg \\
  \textsl{\small Center for Cosmology and Particle Physics,
                 New York University}}
\date{2012 September 11}

\newcommand{\conclusion}{
\blackonwhite
\setbeamercolor{background canvas}{bg=pink}
\begin{frame}
  \frametitle{Punchlines}
  \begin{itemize}
  \item The map--reduce framework (or something like it) does
    complicated tasks in $\log N$ time; it is the ``only'' framework
    for big data operations at the present day.
  \item The next generation of astronomy projects have to go beyond
    maximum-likelihood methods to more responsible probabilistic
    methods if they are going to deliver on their promises.
    \begin{itemize}
    \item \gaia, \lsst, \euclid, \etc
    \end{itemize}
  \item We don't know how to do much beyond maximum likelihood
    ``at scale''.
    \begin{itemize}
    \item call to arms
    \item (and get rich too!)
    \end{itemize}
  \end{itemize}
\end{frame}
\blackonwhite
}

\begin{document}

\blackonwhite

\begin{frame}
  \titlepage
\end{frame}

\conclusion

\begin{frame}
  \frametitle{Principal collaborators}
  \begin{itemize}
  \item Rob Fergus (NYU)
  \item Dan Foreman-Mackey (NYU)
  \item Dustin Lang (CMU)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{map--reduce or die}
  \begin{itemize}
  \item \emph{``We won't even consider any algorithms that can't be
    written in the map--reduce framework.''}
  \item map:
    \begin{itemize}
    \item at each ``data point'' (on the distributed system), do an
      operation on that datum, produce output
    \item think: \emph{Search document for ``kittens''; return
      DocumentID and PageRank if it hits.}
    \item \emph{distributed data} is the key: Every datum is near a
      CPU.
    \end{itemize}
  \item reduce:
    \begin{itemize}
    \item between each pair of outputs, do an operation and return one
      new output, recurse up the tree
    \item think: \emph{Compare two PageRanks and return DocumentID and
      PageRank of the better.}
    \item tree structure of the data center is the key: There are only
      $\log_2 N$ branches to any datum.
    \end{itemize}
  \item Brilliant.  And a huge opportunity.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{maximum-likelihood and map--reduce}
  \begin{itemize}
  \item full-data likelihood: $\displaystyle p(D\given\theta) =
    \prod_n p(d_n\given\theta)$
  \item Find the \emph{maximum with respect to $\theta$} of this
    likelihood.
  \item map:
    \begin{itemize}
    \item compute $\displaystyle\frac{\dd \ln p(d_n\given\theta)}{\dd\theta}$
    \end{itemize}
  \item reduce:
    \begin{itemize}
    \item pairwise sum
    \end{itemize}
  \item Go uphill.  Repeat as necessary; each iteration only takes
    $\log N$ time.
    \begin{itemize}
    \item (use L-BFGS or whatever you like)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{astronomical scale}
  \begin{itemize}
  \item \lsst: $10^{10}$ galaxies in $10^{15}$ pixels
    \begin{itemize}
    \item get the cosmic shear map
    \item and then the cosmological parameters
    \end{itemize}
  \item \gaia: $10^{9}$ stars in $10^{12}$ pixels
    \begin{itemize}
    \item infer the dynamics of the Milky Way
    \item but also---necessarily---the distribution function of stars
      in that potential
    \end{itemize}
  \item \emph{non-parametric} shear map or distribution function
    \begin{itemize}
    \item ``non-parametric'' means the model \emph{gets bigger as the
      data set gets bigger} (or better)
    \item think: \emph{As you observe more and more galaxies, with
      better redshift estimates, you increase the angular and redshift
      resolution of your shear map.}
    \item importantly, non-parametric models are \emph{never} inferred
      at high signal-to-noise
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{graphical models}
\end{frame}

\begin{frame}
  \frametitle{astrophysics problems are hierarchical}
\end{frame}

\begin{frame}
  \frametitle{there are no linear problems}
  \begin{itemize}
  \item Even if your noise is Gaussian, you never know the noise
    variance at high precision.
  \item In most real situations, the data are produced by a
    \emph{mixture of processes}.
  \item There are always multiple modes to the likelihood function,
    and broad support in parameter space.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayesian inference \emph{looks} map--reduce}
  \begin{itemize}
  \item $\displaystyle p(\theta\given D) = \frac{1}{Z}\,p(\theta)\,\prod_n p(d_n\given\theta)$
  \item map:
    \begin{itemize}
    \item compute $p(d_n\given\theta)$
    \end{itemize}
  \item reduce:
    \begin{itemize}
    \item product them all together (starting with the prior)
    \end{itemize}
  \item but think about how you pass forward those functions $p(d_n\given\theta)$
    \begin{itemize}
    \item $\theta$ has $10^6$ or more parameters
    \item functions are multi-modal
    \item support is broader than Gaussian
    \item and non-parametrics are deadly
    \end{itemize}
  \item But that's not all\ldots
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{marginalization is hard---and unavoidable}
  \begin{itemize}
  \item foo
  \end{itemize}
\end{frame}

\conclusion

\end{document}
