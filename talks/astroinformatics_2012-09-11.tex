% Copyright 2012 David W. Hogg (NYU).  All rights reserved.

\documentclass[pdftex]{beamer}
\input{./hogg_presentation} % hogg standard colors
\usepackage{amssymb,amsmath,mathrsfs}

\title{Going beyond map--reduce and \\ going beyond maximum-likelihood}
\author[David W. Hogg (NYU)]{David W. Hogg \\
  \textsl{\small Center for Cosmology and Particle Physics,
                 New York University}}
\date{2012 September 11}

\newcommand{\conclusion}{
\begin{frame}
  \frametitle{punchlines}
  \begin{itemize}
  \item The map--reduce framework (or something like it) does
    important tasks in $\log N$ time; it is the ``only'' framework
    for big data operations at the present day.
    \begin{itemize}
    \item good news: We can do maximum-likelihood problems in map--reduce!
    \end{itemize}
  \item bad news:  The next generation of astronomy projects must go beyond
    maximum-likelihood methods to deliver.
    \begin{itemize}
    \item \gaia, \lsst, \euclid, \etc
    \end{itemize}
  \item We don't know how to do this ``at scale''.
    \begin{itemize}
    \item call to arms
    \item (and get rich too!)
    \end{itemize}
  \end{itemize}
\end{frame}
}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\conclusion

\begin{frame}
  \frametitle{principal collaborators}
  \begin{itemize}
  \item Jo Bovy (IAS)
  \item Brendon Brewer (Auckland)
  \item Rob Fergus (NYU)
  \item Dan Foreman-Mackey (NYU)
  \item Jonathan Goodman (NYU)
  \item Dustin Lang (CMU)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{map--reduce or die}
  \begin{itemize}
  \item \emph{``We won't even consider any algorithms that can't be
    written in the map--reduce framework.''}
  \item map:
    \begin{itemize}
    \item at each ``data point'' (on the distributed system), do an
      operation on that datum, produce output
    \item think: \emph{Search document for ``kittens''; return
      DocumentID and PageRank if it hits.}
    \item \emph{distributed data} is the key: Every datum is near a
      CPU.
    \end{itemize}
  \item reduce:
    \begin{itemize}
    \item between each pair of outputs, do an operation and return one
      new output, recurse up the tree
    \item think: \emph{Compare two PageRanks and return DocumentID and
      PageRank of the better.}
    \item tree structure of the data center is the key: There are only
      $\log_2 N$ branches to any datum.
    \end{itemize}
  \item Brilliant.  And a huge opportunity.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{maximum-likelihood and map--reduce}
  \begin{itemize}
  \item full-data likelihood: $\displaystyle p(D\given\theta) =
    \prod_n p(d_n\given\theta)$
  \item Find the \emph{maximum with respect to $\theta$} of this
    likelihood.
  \item map:
    \begin{itemize}
    \item compute $\displaystyle\frac{\dd \ln p(d_n\given\theta)}{\dd\theta}$
    \end{itemize}
  \item reduce:
    \begin{itemize}
    \item pairwise sum
    \end{itemize}
  \item Go uphill.  Repeat as necessary; each iteration only takes
    $\log N$ time.
    \begin{itemize}
    \item (use L-BFGS or whatever you like)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{astronomical scale}
  \begin{itemize}
  \item \lsst: $10^{10}$ galaxies in $10^{15}$ pixels
    \begin{itemize}
    \item get the cosmic shear map
    \item and then the cosmological parameters
    \end{itemize}
  \item \gaia: $10^{9}$ stars in $10^{12}$ pixels
    \begin{itemize}
    \item infer the dynamics of the Milky Way
    \item but also---necessarily---the distribution function of stars
      in that potential
    \end{itemize}
  \item \emph{non-parametric} shear map or distribution function
    \begin{itemize}
    \item ``non-parametric'' means the model \emph{gets bigger as the
      data set gets bigger} (or better)
    \item think: \emph{As you observe more and more galaxies, with
      better redshift estimates, you increase the angular and redshift
      resolution of your shear map.}
    \item importantly, non-parametric models are \emph{never} inferred
      at high signal-to-noise
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{probabilistic graphical models}
  \includegraphics[clip=true,trim=1in 6in 2in 2.25in]{../pgm/abc.pdf}\\
  $p(a, b, \{c_n\}) = p(a)\,p(b\given a)\,\prod_{n=1}^N p(c_n\given b)$
\end{frame}

\begin{frame}
  \frametitle{astrophysics problems are hierarchical}
  \includegraphics[clip=true,trim=1in 6in 2in 2.25in]{../pgm/gaia.pdf}
\end{frame}

\begin{frame}
  \frametitle{astrophysics problems are hierarchical}
  \includegraphics[clip=true,trim=1in 6in 2in 2.25in]{../pgm/wl.pdf}
\end{frame}

\begin{frame}
  \frametitle{there are no linear problems}
  \begin{itemize}
  \item Even if your noise is Gaussian, you never know the noise
    variance at high precision.
  \item In most real situations, the data are produced by a
    \emph{mixture of processes}.
  \item There are always multiple modes to the likelihood function,
    and broad support in parameter space.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayesian inference \emph{isn't} map--reduce}
  \begin{itemize}
  \item $\displaystyle p(\theta\given D) = \frac{1}{Z}\,p(\theta)\,\prod_n p(d_n\given\theta)$
  \item map:
    \begin{itemize}
    \item compute functions $p(d_n\given\theta)$
    \end{itemize}
  \item reduce:
    \begin{itemize}
    \item product functions together (starting with the prior)
    \end{itemize}
  \item but think about how you pass forward those functions
    \begin{itemize}
    \item $\theta$ has $10^6$ or more parameters
    \item functions are multi-modal
    \item support is broader than Gaussian
    \item and non-parametrics are deadly
    \end{itemize}
  \item But that's not all\ldots
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{marginalization is hard---and unavoidable}
  \includegraphics[clip=true,trim=1in 6in 2in 2.25in]{../pgm/wl.pdf}
\end{frame}

\begin{frame}
  \frametitle{Bayesian state-of-the-art}
  \begin{itemize}
  \item there \emph{are} huge non-parametric Bayesian inferences with
    massive marginalizations out there
  \item How were they done?
    \begin{itemize}
    \item carefully chosen priors that make the inferences and
      marginalizations analytic or tractable
    \item \emph{we can't do this}
    \item why not?  Because for us the priors \emph{actually are} our
      prior beliefs.  Our prior beliefs are not conjugate to anything!
    \end{itemize}
  \item ``Bayesian'' is becoming a bad word
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{my approach}
  \begin{itemize}
  \item brute force
    \begin{itemize}
    \item (plus some help from applied math and computer vision)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{My day job}
  \begin{itemize}
  \item Lang \& Hogg {\small(forthcoming)}: a $10^9$-parameter model of the
    $10^{13}$ \sdss\ pixels (\project{The Tractor})
  \item Brewer \etal\ {\small(forthcoming)}: Bayesian non-parametrics but with
    priors that represent our actual prior knowledge
  \item Foreman-Mackey \etal\ {\small(arXiv:1202.3665)}:
    \project{emcee}, the MCMC Hammer: flexible, parallelized, adaptive
    sampler
  \item Bovy, Murray, Hogg {\small(arXiv:0903.5308)}: a dynamical inference
    fully marginalizing out a non-parametric distribution function
  \item Bovy \etal\ {\small(arXiv:1105.3975)}: a 60,000-parameter model of
    700,000 flux measurements, followed by predictions for 160,000,000
    point sources
  \item Bovy, Hogg, Roweis {\small(arXiv:0905.2979)}: \project{extreme
    deconvolution}: hierarchical inference in the presence of missing
    data and heterogeneous noise
  \end{itemize}
\end{frame}

\conclusion

\end{document}
